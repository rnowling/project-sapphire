{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: Identify portions of text that have been copied from a Wikipedia article in one language to an article in another language.\n",
      "\n",
      "To achieve this goal, we need a method that can match pieces of text.  The method should account for the following:\n",
      "\n",
      "* Word order may vary locally (e.g., \"the painted red house\" can become \"the house painted red\").\n",
      "* Individual words may be dropped, added, or changed (e.g., \"the painted red house\" can become \"the red colored house\").\n",
      "* Units of copied text may be as small as a sentence or clause.\n",
      "\n",
      "To meet the criteria, we propose classifying matching each word using its $(N_w - 1)/2$ neighboring words on its left and right. In other words, we propose using a sliding window of $N_w$ linearly-located words to classify each word.  The group of words will be represented as an unordered set to handle local variations in word order.\n",
      "\n",
      "To handle misspellings, we may want to cluster words by their edit distances.\n",
      "\n",
      "Two classification methods appear promising:\n",
      "\n",
      "* Nearest neighbor using cosine similarity -- we can either use word occurence or weight words by their inverse-frequency\n",
      "* Bernoulli Naive Bayes (we don't consider word counts)\n",
      "\n",
      "We need ways to evaluate the choices of $N_w$ and the classification method.  \n",
      "\n",
      "Test 1\n",
      "------\n",
      "Pairwise similarity of windows.\n",
      "\n",
      "Test 2\n",
      "------\n",
      "Assess how well the classifier separates the true match from other matches.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets of length $N_w$.  Compute scores between all pairs of word sets.  Histogram scores for all matches except true matches.\n",
      "\n",
      "Test 3\n",
      "------\n",
      "Assess the likelihood of similar matches occurring randomly.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets $S_t$ of length $N_w$. Compute the frequencies of each word and generate fake word sets $S_f$ by drawing with replacement from the computed distribution. Compute scores between all pairs of word sets.  Histogram scores.\n",
      "\n",
      "Test 4\n",
      "------\n",
      "Assess the effect of random mutations -- how well does the method work when one or more of the words are changed?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparing Test Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The English version of the Wikipedia articles can be downloaded [here](http://dumps.wikimedia.org/enwiki/).  I used the smallest file of the complete pages, current versions only, in XML format, bzipped.\n",
      "\n",
      "1. Parse XML files\n",
      "2. Strip mediawiki formatting\n",
      "3. Remove punctuation and make letters lowercase\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "def read_articles(flname):\n",
      "    tree = ET.parse(flname)\n",
      "    root = tree.getroot()\n",
      "    formatted_articles = dict()\n",
      "    for page in root.iterfind(\"{http://www.mediawiki.org/xml/export-0.8/}page\"):\n",
      "        article_text = None\n",
      "        title = page.find(\"{http://www.mediawiki.org/xml/export-0.8/}title\").text\n",
      "        revision = page.find(\"{http://www.mediawiki.org/xml/export-0.8/}revision\")\n",
      "        text = revision.find(\"{http://www.mediawiki.org/xml/export-0.8/}text\").text\n",
      "        formatted_articles[title] = text\n",
      "    return formatted_articles\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def remove_block(text, start_delim, end_delim):\n",
      "    i = 0\n",
      "    new_string = \"\"\n",
      "    while True:\n",
      "        start = text.find(start_delim, i)\n",
      "        if start == -1:\n",
      "            break\n",
      "        end = text.find(end_delim, start+1)\n",
      "        if i != start:\n",
      "            new_string += text[i:start]\n",
      "        i = end + len(end_delim)\n",
      "    new_string += text[i:]\n",
      "    \n",
      "    return new_string\n",
      "\n",
      "def remove_match(text, query):\n",
      "    i = 0\n",
      "    new_string = \"\"\n",
      "    while True:\n",
      "        start = text.find(query, i)\n",
      "        if start == -1:\n",
      "            break\n",
      "        if i != start:\n",
      "            new_string += text[i:start]\n",
      "        i = start + len(query)\n",
      "    new_string += text[i:]\n",
      "    \n",
      "    return new_string\n",
      "\n",
      "def replace_link_block(block):\n",
      "    if block.startswith(\"[[image:\") or \\\n",
      "        block.startswith(\"[[file:\"):\n",
      "            return \" \"\n",
      "    elif block[1] != \"[\":\n",
      "        start = block.find(\" \")\n",
      "        if start == -1:\n",
      "            return \" \"\n",
      "        else:\n",
      "            return block[start+1:-1]\n",
      "    elif \"|\" in block:\n",
      "        start = block.find(\"|\")\n",
      "        return block[start+1:-2]\n",
      "    \n",
      "    return block[2:-2]\n",
      "\n",
      "def substitute_links(text):\n",
      "    pos = 0\n",
      "    start_pos = 0\n",
      "    while pos < len(text):\n",
      "        if text[pos:pos+2] == \"[[\":\n",
      "            start_pos = pos\n",
      "            pos += 2\n",
      "        elif text[pos] == \"[\":\n",
      "            start_pos = pos\n",
      "            pos += 1\n",
      "        elif text[pos:pos+2] == \"]]\":\n",
      "            block = text[start_pos:pos+2]\n",
      "            block = replace_link_block(block)\n",
      "            text = text[:start_pos] + block + text[pos+2:]\n",
      "            pos = 0\n",
      "        elif text[pos] == \"]\":\n",
      "            block = text[start_pos:pos+1]\n",
      "            block = replace_link_block(block)\n",
      "            text = text[:start_pos] + block + text[pos+1:]\n",
      "            pos = 0\n",
      "        else:\n",
      "            pos += 1\n",
      "            \n",
      "    return text\n",
      "\n",
      "def remove_tail(text):\n",
      "    headers = [\"==See Also==\", \"== See Also ==\", \"==References==\", \"== References ==\",\n",
      "                \"==External Links==\", \"== External Links ==\", \"==Further Reading==\",\n",
      "                \"== Further Reading ==\"]\n",
      "    positions = [text.find(s.lower()) for s in headers]\n",
      "    positions = filter(lambda p: p != -1, positions)\n",
      "    \n",
      "    if len(positions) == 0:\n",
      "        return text\n",
      "    \n",
      "    pos = min(positions)\n",
      "    \n",
      "    return text[:pos]\n",
      "\n",
      "TAG_REGEX = re.compile(r\"<(\\w+).+?/\\1\\s*>\", flags=re.DOTALL)\n",
      "\n",
      "def remove_html_tags(text):\n",
      "    pos = 0\n",
      "    start_pos = 0\n",
      "    last_block_end = 0\n",
      "    new_string = \"\"\n",
      "    while pos < len(text):\n",
      "        if text[pos] == \"<\":\n",
      "            start_pos = pos\n",
      "        elif text[pos:pos+2] == \"/>\":\n",
      "            new_string += text[last_block_end:start_pos] + \" \"\n",
      "            pos += 2\n",
      "            last_block_end = pos\n",
      "        pos += 1\n",
      "    new_string += text[last_block_end:]\n",
      "    text = new_string\n",
      "    \n",
      "    new_string = \"\"\n",
      "    last_end = 0\n",
      "    for match in TAG_REGEX.finditer(text):\n",
      "        new_string += text[last_end:match.start(0)] + \" \" \n",
      "        last_end = match.end(0)\n",
      "    new_string += text[last_end:]\n",
      "    \n",
      "    return new_string\n",
      "\n",
      "def replace(text, query, sub):\n",
      "    i = 0\n",
      "    new_string = \"\"\n",
      "    while True:\n",
      "        start = text.find(query, i)\n",
      "        if start == -1:\n",
      "            break\n",
      "        if i != start:\n",
      "            new_string += text[i:start]\n",
      "        new_string += sub\n",
      "        i = start + len(query)\n",
      "    new_string += text[i:]\n",
      "    \n",
      "    return new_string\n",
      "        \n",
      "\n",
      "def remove_wiki_formatting(text):\n",
      "    text = text.lower()\n",
      "    \n",
      "    text = remove_tail(text)\n",
      "    \n",
      "    text = remove_block(text, \"{{\", \"}}\")\n",
      "    text = substitute_links(text)\n",
      "    \n",
      "    text = remove_block(text, \"<!--\", \"-->\")\n",
      "    text = remove_html_tags(text)\n",
      "   \n",
      "    text = replace(text, \"-\", \" \")\n",
      "    text = replace(text, \"&nbsp;\", \" \")\n",
      "    text = replace(text, \"&gt;\", \">\")\n",
      "    text = replace(text, \"&lt;\", \"<\")\n",
      "    text = replace(text, \"&amp;\", \" \")\n",
      "    text = replace(text, \"|\", \" \")\n",
      "    \n",
      "    text = remove_match(text, \"====\")\n",
      "    text = remove_match(text, \"===\")\n",
      "    text = remove_match(text, \"==\")\n",
      "    text = remove_match(text, \"=\")\n",
      "    text = remove_match(text, \"'''\")\n",
      "    text = remove_match(text, \"''\")\n",
      "    text = remove_match(text, \".\")\n",
      "    text = remove_match(text, \",\")\n",
      "    text = remove_match(text, \"\\\"\")\n",
      "    text = remove_match(text, \"(\")\n",
      "    text = remove_match(text, \")\")\n",
      "    text = remove_match(text, \"'\")\n",
      "    text = remove_match(text, \"?\")\n",
      "    text = remove_match(text, \";\")\n",
      "    text = remove_match(text, \":\")\n",
      "    text = remove_match(text, \"!\")\n",
      "    text = remove_match(text, \"*\")\n",
      "    \n",
      "    return \" \".join(text.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = read_articles(\"data/enwiki-20140707-pages-meta-current1.xml-p000000010p000010000\")\n",
      "articles = filter(lambda t: not t[0].startwith(\"Talk:\"), articles.iteritems())\n",
      "articles = filter(lambda t: len(t[1]) > 5000, articles.iteritems())\n",
      "articles = random.sample(articles, 10)\n",
      "articles = map(lambda t: (t[0], remove_wiki_formatting(t[1])), articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Extraction of Word Sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_word_sets(article, window_length):\n",
      "    title, text = article\n",
      "    words = text.split()\n",
      "    word_sets = list()\n",
      "    for i in xrange(len(words) - window_length):\n",
      "        word_set = (frozenset(words[i:i+window_length]), title, i + ((window_length - 1) / 2))\n",
      "        word_sets.append(word_set)\n",
      "    return word_sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_sets_3 = reduce(lambda x, y: x + y, map(lambda article: extract_word_sets(article, 3), articles))\n",
      "word_sets_5 = reduce(lambda x, y: x + y, map(lambda article: extract_word_sets(article, 5), articles))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(word_sets_3), len(word_sets_5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "28404 28384 28364\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print word_sets_3[0]\n",
      "print word_sets_5[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(frozenset([u'}}', u'cuauht\\xe9moc', u'also']), u'Cuauht\\xe9moc', 1)\n",
        "(frozenset([u'}}', u'cuauht\\xe9moc', u'as', u'known', u'also']), u'Cuauht\\xe9moc', 2)\n",
        "(frozenset([u'cuauhtemotzin', u'also', u'}}', u'cuauht\\xe9moc', u'as', u'guatimozin', u'known']), u'Cuauht\\xe9moc', 3)\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bernoulli Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
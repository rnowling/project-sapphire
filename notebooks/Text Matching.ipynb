{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: Identify portions of text that have been copied from a Wikipedia article in one language to an article in another language.\n",
      "\n",
      "To achieve this goal, we need a method that can match pieces of text.  The method should account for the following:\n",
      "\n",
      "* Word order may vary locally (e.g., \"the painted red house\" can become \"the house painted red\").\n",
      "* Individual words may be dropped, added, or changed (e.g., \"the painted red house\" can become \"the red colored house\").\n",
      "* Units of copied text may be as small as a sentence or clause.\n",
      "\n",
      "To meet the criteria, we propose classifying matching each word using its $(N_w - 1)/2$ neighboring words on its left and right. In other words, we propose using a sliding window of $N_w$ linearly-located words to classify each word.  The group of words will be represented as an unordered set to handle local variations in word order.\n",
      "\n",
      "To handle misspellings, we may want to cluster words by their edit distances.\n",
      "\n",
      "Two classification methods appear promising:\n",
      "\n",
      "* Nearest neighbor using cosine similarity -- we can either use word occurence or weight words by their inverse-frequency\n",
      "* Bernoulli Naive Bayes (we don't consider word counts)\n",
      "\n",
      "We need ways to evaluate the choices of $N_w$ and the classification method.  \n",
      "\n",
      "Test 1\n",
      "------\n",
      "Determine how well the method distinguishes between the true match and similar matches.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets of length $N_w$.  Compute scores between all pairs of word sets.  Histogram scores for all matches except true matches.\n",
      "\n",
      "Test 2\n",
      "------\n",
      "Determine if the features are stastically significant.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets $S_t$ of length $N_w$. Compute the frequencies of each word and generate fake word sets $S_f$ by drawing with replacement from the computed distribution. Compute scores between all pairs of word sets.  Histogram scores.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
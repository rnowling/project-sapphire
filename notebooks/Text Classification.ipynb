{
 "metadata": {
  "name": "",
  "signature": "sha256:67f0a775a64b777dcd2a6eb29750ba294433db15f67719b247a045dbe4ed3d5d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: Identify portions of text that have been copied from a Wikipedia article in one language to an article in another language.\n",
      "\n",
      "To achieve this goal, we need a method that can match pieces of text.  The method should account for the following:\n",
      "\n",
      "* Word order may vary locally (e.g., \"the painted red house\" can become \"the house painted red\").\n",
      "* Individual words may be dropped, added, or changed (e.g., \"the painted red house\" can become \"the red colored house\").\n",
      "* Units of copied text may be as small as a sentence or clause.\n",
      "\n",
      "To meet the criteria, we propose classifying matching each word using its $(N_w - 1)/2$ neighboring words on its left and right. In other words, we propose using a sliding window of $N_w$ linearly-located words to classify each word.  The group of words will be represented as an unordered set to handle local variations in word order.\n",
      "\n",
      "To handle misspellings, we may want to cluster words by their edit distances.\n",
      "\n",
      "Two classification methods appear promising:\n",
      "\n",
      "* Nearest neighbor using cosine similarity -- we can either use word occurence or weight words by their inverse-frequency\n",
      "* Bernoulli Naive Bayes (we don't consider word counts)\n",
      "\n",
      "We need ways to evaluate the choices of $N_w$ and the classification method.  \n",
      "\n",
      "Test 1\n",
      "------\n",
      "Pairwise similarity of windows.\n",
      "\n",
      "Test 2\n",
      "------\n",
      "Assess how well the classifier separates the true match from other matches.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets of length $N_w$.  Compute scores between all pairs of word sets.  Histogram scores for all matches except true matches.\n",
      "\n",
      "Test 3\n",
      "------\n",
      "Assess the likelihood of similar matches occurring randomly.\n",
      "\n",
      "Compute distribution of wikipedia article word-lengths.  Randomly sample $N$ (maybe 100 or 500) articles.  Extract all word sets $S_t$ of length $N_w$. Compute the frequencies of each word and generate fake word sets $S_f$ by drawing with replacement from the computed distribution. Compute scores between all pairs of word sets.  Histogram scores.\n",
      "\n",
      "Test 4\n",
      "------\n",
      "Assess the effect of random mutations -- how well does the method work when one or more of the words are changed?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparing Test Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The English version of the Wikipedia articles can be downloaded [here](http://dumps.wikimedia.org/enwiki/).  I used the smallest file of the complete pages, current versions only, in XML format, bzipped.\n",
      "\n",
      "1. Parse XML files\n",
      "2. Strip mediawiki formatting\n",
      "3. Remove punctuation and make letters lowercase\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step is to extract the articles from the XML dump."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "def read_articles(flname):\n",
      "    tree = ET.parse(flname)\n",
      "    root = tree.getroot()\n",
      "    formatted_articles = []\n",
      "    for page in root.iterfind(\"{http://www.mediawiki.org/xml/export-0.8/}page\"):\n",
      "        article_text = None\n",
      "        title = page.find(\"{http://www.mediawiki.org/xml/export-0.8/}title\").text\n",
      "        revision = page.find(\"{http://www.mediawiki.org/xml/export-0.8/}revision\")\n",
      "        text = revision.find(\"{http://www.mediawiki.org/xml/export-0.8/}text\").text\n",
      "        formatted_articles.append((title, text))\n",
      "    return formatted_articles\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = read_articles(\"data/enwiki-20140707-pages-meta-current1.xml-p000000010p000010000\")\n",
      "articles = filter(lambda t: not t[0].startswith(\"Talk:\"), articles)\n",
      "print len(articles)\n",
      "articles = filter(lambda t: len(t[1]) > 5000, articles)\n",
      "print len(articles)\n",
      "articles = random.sample(articles, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6317\n",
        "3942\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second step is to remove the mediawiki formatting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def remove_block_regex(text, start_delim, end_delim):\n",
      "    block_regex = re.compile(start_delim + r\".+?\" + end_delim, flags=re.DOTALL)\n",
      "    new_string = \"\"\n",
      "    last_end = 0\n",
      "    for match in block_regex.finditer(text):\n",
      "        new_string += text[last_end:match.start(0)] + \" \" \n",
      "        last_end = match.end(0)\n",
      "    new_string += text[last_end:]\n",
      "    \n",
      "    return new_string\n",
      "\n",
      "def replace_link_block(block):\n",
      "    if block.startswith(\"[[image:\") or \\\n",
      "        block.startswith(\"[[file:\"):\n",
      "            return \" \"\n",
      "    elif block[1] != \"[\":\n",
      "        start = block.find(\" \")\n",
      "        if start == -1:\n",
      "            return \" \"\n",
      "        else:\n",
      "            return block[start+1:-1]\n",
      "    elif \"|\" in block:\n",
      "        start = block.find(\"|\")\n",
      "        return block[start+1:-2]\n",
      "    \n",
      "    return block[2:-2]\n",
      "\n",
      "def substitute_links(text):\n",
      "    pos = 0\n",
      "    start_pos = 0\n",
      "    while pos < len(text):\n",
      "        if text[pos:pos+2] == \"[[\":\n",
      "            start_pos = pos\n",
      "            pos += 2\n",
      "        elif text[pos] == \"[\":\n",
      "            start_pos = pos\n",
      "            pos += 1\n",
      "        elif text[pos:pos+2] == \"]]\":\n",
      "            block = text[start_pos:pos+2]\n",
      "            block = replace_link_block(block)\n",
      "            text = text[:start_pos] + block + text[pos+2:]\n",
      "            pos = 0\n",
      "        elif text[pos] == \"]\":\n",
      "            block = text[start_pos:pos+1]\n",
      "            block = replace_link_block(block)\n",
      "            text = text[:start_pos] + block + text[pos+1:]\n",
      "            pos = 0\n",
      "        else:\n",
      "            pos += 1\n",
      "            \n",
      "    return text\n",
      "\n",
      "def remove_tail(text):\n",
      "    headers = [\"==See Also==\", \"== See Also ==\", \"==References==\", \"== References ==\",\n",
      "                \"==External Links==\", \"== External Links ==\", \"==Further Reading==\",\n",
      "                \"== Further Reading ==\"]\n",
      "    positions = [text.find(s.lower()) for s in headers]\n",
      "    positions = filter(lambda p: p != -1, positions)\n",
      "    \n",
      "    if len(positions) == 0:\n",
      "        return text\n",
      "    \n",
      "    pos = min(positions)\n",
      "    \n",
      "    return text[:pos]\n",
      "\n",
      "TAG_REGEX = re.compile(r\"<\\s*(\\w+).+?/\\1\\s*>\", flags=re.DOTALL)\n",
      "\n",
      "def remove_html_tags(text):\n",
      "    pos = 0\n",
      "    start_pos = 0\n",
      "    last_block_end = 0\n",
      "    new_string = \"\"\n",
      "    while pos < len(text):\n",
      "        if text[pos] == \"<\":\n",
      "            start_pos = pos\n",
      "        elif text[pos:pos+2] == \"/>\":\n",
      "            new_string += text[last_block_end:start_pos] + \" \"\n",
      "            pos += 2\n",
      "            last_block_end = pos\n",
      "        pos += 1\n",
      "    new_string += text[last_block_end:]\n",
      "    text = new_string\n",
      "    \n",
      "    new_string = \"\"\n",
      "    last_end = 0\n",
      "    for match in TAG_REGEX.finditer(text):\n",
      "        new_string += text[last_end:match.start(0)] + \" \" \n",
      "        last_end = match.end(0)\n",
      "    new_string += text[last_end:]\n",
      "    \n",
      "    return new_string\n",
      "        \n",
      "\n",
      "def remove_wiki_formatting(text):\n",
      "    text = text.lower()\n",
      "    \n",
      "    text = remove_tail(text)\n",
      "    \n",
      "    text = remove_block_regex(text, r\"\\{\\{\", r\"\\}\\}\")\n",
      "    text = substitute_links(text)\n",
      "    \n",
      "    text = remove_block_regex(text, r\"<!--\", r\"-->\")\n",
      "    text = remove_html_tags(text)\n",
      "   \n",
      "    text = text.replace(\">\", \" \")\n",
      "    text = text.replace(\"<\", \" \")\n",
      "    text = text.replace(\"-\", \" \")\n",
      "    text = text.replace(\"&nbsp;\", \" \")\n",
      "    text = text.replace(\"&gt;\", \">\")\n",
      "    text = text.replace(\"&lt;\", \"<\")\n",
      "    text = text.replace(\"&amp;\", \" \")\n",
      "    text = text.replace(\"|\", \" \")\n",
      "    text = text.replace(\"/\", \" \")\n",
      "    text = text.replace(\"}}\", \" \")\n",
      "    text = text.replace(\"{{\", \" \")\n",
      "    \n",
      "    text = text.replace(\"===\", \"\")\n",
      "    text = text.replace(\"==\", \"\")\n",
      "    text = text.replace(\"=\", \"\")\n",
      "    text = text.replace(\"'''\", \"\")\n",
      "    text = text.replace(\"''\", \"\")\n",
      "    text = text.replace(\".\", \"\")\n",
      "    text = text.replace(\",\", \"\")\n",
      "    text = text.replace(\"\\\"\", \"\")\n",
      "    text = text.replace(\"(\", \"\")\n",
      "    text = text.replace(\")\", \"\")\n",
      "    text = text.replace(\"'\", \"\")\n",
      "    text = text.replace(\"?\", \"\")\n",
      "    text = text.replace(\";\", \"\")\n",
      "    text = text.replace(\":\", \"\")\n",
      "    text = text.replace(\"!\", \"\")\n",
      "    text = text.replace(\"*\", \"\")\n",
      "    \n",
      "    return \" \".join(text.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = map(lambda t: (t[0], remove_wiki_formatting(t[1])), articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Extraction of Word Sets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have each article in unformatted, plain text, we can extract the unordered word sets.  The word sets are found by scanning over the articles' text with a sliding window.  For each word, we extract the word, its $n$ neighbors to the left, and its $n$ neighbors to the right.  We represent each word set as a tuple containing an unordered set of the extracted words, the index of the center word, and the title of the article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_word_sets(article, window_length):\n",
      "    title, text = article\n",
      "    words = text.split()\n",
      "    word_sets = list()\n",
      "    for i in xrange(len(words) - window_length):\n",
      "        word_set = (frozenset(words[i:i+window_length]), title, i + ((window_length - 1) / 2))\n",
      "        word_sets.append(word_set)\n",
      "    return word_sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "article_word_sets_3 = map(lambda article: extract_word_sets(article, 3), articles)\n",
      "article_word_sets_5 = map(lambda article: extract_word_sets(article, 5), articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_sets_3 = reduce(lambda x, y: x + y, article_word_sets_3)\n",
      "word_sets_5 = reduce(lambda x, y: x + y, article_word_sets_5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(word_sets_3), len(word_sets_5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "90222 90172\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print word_sets_3[0]\n",
      "print word_sets_5[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(frozenset([u'release', u'discontinued', u'latest']), 'Cyc', 1)\n",
        "(frozenset([u'release', u'version', u'discontinued', u'40', u'latest']), 'Cyc', 2)\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bernoulli Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}